# Rust í•œêµ­ ë²•ë¥  ë¬¸ì„œ íŒŒì„œ ì„¤ê³„ì„œ

> Korean Legal Document Parser Design Specification
>
> ë²„ì „: 1.0.0
> ì‘ì„±ì¼: 2026-01-18
> ì‘ì„±ì: Claude Code

---

## 1. ê°œìš”

### 1.1 ëª©ì 

Pythonìœ¼ë¡œ êµ¬í˜„ëœ `legal_chunker.py`ì˜ í•œêµ­ ë²•ë¥  ë¬¸ì„œ íŒŒì‹± ë¡œì§ì„ Rustë¡œ í¬íŒ…í•˜ì—¬ ì„±ëŠ¥ì„ ê°œì„ í•˜ê³ , ê¸°ì¡´ `markdown-media` Rust íŒŒì„œì˜ HWP í…ìŠ¤íŠ¸ ì¶”ì¶œ ë²„ê·¸ë¥¼ ìˆ˜ì •í•œë‹¤.

### 1.2 ë²”ìœ„

1. **HWP íŒŒì„œ ë²„ê·¸ ìˆ˜ì •**: í™•ì¥ ì œì–´ ë¬¸ì(0x16-0x1F) ì²˜ë¦¬ ë¡œì§ ìˆ˜ì •
2. **í•œêµ­ ë²•ë¥  ì²­ì»¤ êµ¬í˜„**: ë²•ë ¹ êµ¬ì¡° íŒŒì‹± ë° ì˜ë¯¸ì  ì²­í‚¹
3. **JSONL ë‚´ë³´ë‚´ê¸°**: WeKnora RAG ì‹œìŠ¤í…œìš© ì¶œë ¥ í¬ë§·

### 1.3 ìš©ì–´ ì •ì˜

| ìš©ì–´ | ì •ì˜ |
|------|------|
| HWP | í•œê¸€ê³¼ì»´í“¨í„°ì˜ ë¬¸ì„œ íŒŒì¼ í˜•ì‹ (Hangul Word Processor) |
| OLE | Object Linking and Embedding, HWP 5.0ì˜ ì»¨í…Œì´ë„ˆ í˜•ì‹ |
| í™•ì¥ ì œì–´ ë¬¸ì | HWPì—ì„œ 16ë°”ì´íŠ¸(2+14)ë¥¼ ì†Œë¹„í•˜ëŠ” íŠ¹ìˆ˜ ë¬¸ì ì½”ë“œ |
| ì²­í¬(Chunk) | ë²¡í„° ì„ë² ë”©ì„ ìœ„í•´ ë¶„í• ëœ í…ìŠ¤íŠ¸ ë‹¨ìœ„ |
| ì¡°(Article) | í•œêµ­ ë²•ë ¹ì˜ ê¸°ë³¸ ë‹¨ìœ„ (ì˜ˆ: ì œ1ì¡°) |

---

## 2. í˜„í™© ë¶„ì„

### 2.1 ë¬¸ì œì 

#### 2.1.1 Rust íŒŒì„œì˜ í™•ì¥ ì œì–´ ë¬¸ì ì²˜ë¦¬ ë²„ê·¸

**í˜„ì¬ ì½”ë“œ** (`record.rs:231-238`):
```rust
match char_code {
    0x01..=0x08 | 0x0B | 0x0C | 0x0E | 0x0F | 0x10..=0x15 => {
        i = (i + 14).min(data.len());  // 14ë°”ì´íŠ¸ ìŠ¤í‚µ âœ…
    }
    0x16..=0x1F => continue,  // 14ë°”ì´íŠ¸ ìŠ¤í‚µ ì•ˆí•¨ âŒ
}
```

**Python ì½”ë“œ** (`hwp_converter.py:136-139`):
```python
EXTENDED_CTRL_CHARS = {
    0x01, 0x02, ..., 0x15,
    0x16, 0x17, 0x18, 0x19, 0x1A, 0x1B, 0x1C, 0x1D, 0x1E, 0x1F  # ì¶”ê°€
}
if char_code in EXTENDED_CTRL_CHARS:
    i += 14  # ëª¨ë“  í™•ì¥ ì œì–´ ë¬¸ìì—ì„œ 14ë°”ì´íŠ¸ ìŠ¤í‚µ âœ…
```

**ì˜í–¥**: 0x16-0x1F ì½”ë“œê°€ í¬í•¨ëœ HWP íŒŒì¼ì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ ì‹œ ë°ì´í„° ì˜¤í”„ì…‹ì´ ì–´ê¸‹ë‚˜ ì˜ëª»ëœ í…ìŠ¤íŠ¸ê°€ ì¶”ì¶œë¨.

#### 2.1.2 Rustì— ë²•ë¥  êµ¬ì¡° íŒŒì‹± ê¸°ëŠ¥ ë¶€ì¬

Python `legal_chunker.py`ì—ì„œ ì œê³µí•˜ëŠ” ë‹¤ìŒ ê¸°ëŠ¥ì´ Rustì— ì—†ìŒ:
- ë²•ë ¹ ê³„ì¸µ êµ¬ì¡° íŒŒì‹± (í¸/ì¥/ì ˆ/ê´€/ì¡°/í•­/í˜¸/ëª©)
- ì¡°(Article) ë‹¨ìœ„ ì˜ë¯¸ì  ì²­í‚¹
- ë²•ì¡°ë¬¸ ì°¸ì¡° ê´€ê³„ ì¶”ì¶œ
- JSONL ì¶œë ¥

### 2.2 Python íŒŒì„œ ë¶„ì„

#### ë°ì´í„° íë¦„
```
Markdown íŒŒì¼
    â†“
parse_metadata_header() â†’ LegalMetadata ì¶”ì¶œ
    â†“
parse_article_block() â†’ ì¡° ë‹¨ìœ„ íŒŒì‹±
    â†“
chunk_large_article() â†’ í° ì¡°ë¬¸ ë¶„í• 
    â†“
export_to_jsonl() â†’ JSONL ì¶œë ¥
```

#### í•µì‹¬ ë°ì´í„° êµ¬ì¡°
```python
@dataclass
class LegalChunk:
    id: str                    # SHA256 í•´ì‹œ
    content: str               # ì²­í¬ ë‚´ìš©
    metadata: LegalMetadata    # ë©”íƒ€ë°ì´í„°
    chunk_type: str            # article | paragraph | definition
    token_count: int           # í† í° ìˆ˜
    context_path: str          # "ì œ1í¸ > ì œ1ì¥ > ì œ1ì¡°"
    parent_chunk_id: Optional[str]
```

---

## 3. ì•„í‚¤í…ì²˜ ì„¤ê³„

### 3.1 ëª¨ë“ˆ êµ¬ì¡°

```
core/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ hwp/
â”‚   â”‚   â”œâ”€â”€ mod.rs           # ê¸°ì¡´
â”‚   â”‚   â”œâ”€â”€ parser.rs        # ê¸°ì¡´
â”‚   â”‚   â”œâ”€â”€ record.rs        # ğŸ”§ ìˆ˜ì • (í™•ì¥ ì œì–´ ë¬¸ì)
â”‚   â”‚   â””â”€â”€ ole.rs           # ê¸°ì¡´
â”‚   â”‚
â”‚   â”œâ”€â”€ legal/               # ğŸ†• ì‹ ê·œ ëª¨ë“ˆ
â”‚   â”‚   â”œâ”€â”€ mod.rs           # ëª¨ë“ˆ ì„ ì–¸
â”‚   â”‚   â”œâ”€â”€ types.rs         # ë°ì´í„° íƒ€ì… ì •ì˜
â”‚   â”‚   â”œâ”€â”€ patterns.rs      # ì •ê·œì‹ íŒ¨í„´
â”‚   â”‚   â”œâ”€â”€ chunker.rs       # ì²­í‚¹ ë¡œì§
â”‚   â”‚   â””â”€â”€ exporter.rs      # JSONL ë‚´ë³´ë‚´ê¸°
â”‚   â”‚
â”‚   â”œâ”€â”€ lib.rs               # ğŸ”§ ìˆ˜ì • (legal ëª¨ë“ˆ ì¶”ê°€)
â”‚   â””â”€â”€ main.rs              # ğŸ”§ ìˆ˜ì • (CLI ëª…ë ¹ ì¶”ê°€)
â”‚
â”œâ”€â”€ tests/
â”‚   â””â”€â”€ legal_tests.rs       # ğŸ†• í†µí•© í…ŒìŠ¤íŠ¸
â”‚
â””â”€â”€ Cargo.toml               # ğŸ”§ ìˆ˜ì • (ì˜ì¡´ì„± ì¶”ê°€)
```

### 3.2 ì˜ì¡´ì„±

```toml
[dependencies]
# ê¸°ì¡´ ì˜ì¡´ì„±
cfb = "0.7"
flate2 = "1.0"
miniz_oxide = "0.7"
serde = { version = "1.0", features = ["derive"] }
serde_json = "1.0"

# ì‹ ê·œ ì˜ì¡´ì„±
regex = "1.10"
lazy_static = "1.4"
sha2 = "0.10"
```

---

## 4. ìƒì„¸ ì„¤ê³„

### 4.1 HWP íŒŒì„œ ìˆ˜ì • (`record.rs`)

#### 4.1.1 í™•ì¥ ì œì–´ ë¬¸ì ìƒìˆ˜ ìˆ˜ì •

**Before:**
```rust
pub const EXTENDED_CTRL_CHARS: [u16; 18] = [
    0x01, 0x02, 0x03, 0x04, 0x05, 0x06, 0x07, 0x08,
    0x0B, 0x0C, 0x0E, 0x0F,
    0x10, 0x11, 0x12, 0x13, 0x14, 0x15,
];
```

**After:**
```rust
pub const EXTENDED_CTRL_CHARS: [u16; 28] = [
    0x01, 0x02, 0x03, 0x04, 0x05, 0x06, 0x07, 0x08,
    0x0B, 0x0C, 0x0E, 0x0F,
    0x10, 0x11, 0x12, 0x13, 0x14, 0x15,
    0x16, 0x17, 0x18, 0x19, 0x1A, 0x1B, 0x1C, 0x1D, 0x1E, 0x1F,  // ì¶”ê°€
];
```

#### 4.1.2 `extract_para_text()` í•¨ìˆ˜ ìˆ˜ì •

**Before:**
```rust
0x16..=0x1F => continue,
```

**After:**
```rust
0x16..=0x1F => {
    // Extended control characters - skip 14 bytes payload
    i = (i + 14).min(data.len());
}
```

### 4.2 íƒ€ì… ì •ì˜ (`legal/types.rs`)

```rust
use serde::{Deserialize, Serialize};

/// ë²•ë ¹ ê³„ì¸µ êµ¬ì¡°
#[derive(Debug, Clone, Copy, PartialEq, Eq, Serialize, Deserialize)]
#[serde(rename_all = "snake_case")]
pub enum LegalHierarchy {
    Part,        // í¸
    Chapter,     // ì¥
    Section,     // ì ˆ
    SubSection,  // ê´€
    Article,     // ì¡°
    Paragraph,   // í•­
    SubParagraph,// í˜¸
    Item,        // ëª©
}

impl LegalHierarchy {
    pub fn korean_name(&self) -> &'static str {
        match self {
            Self::Part => "í¸",
            Self::Chapter => "ì¥",
            Self::Section => "ì ˆ",
            Self::SubSection => "ê´€",
            Self::Article => "ì¡°",
            Self::Paragraph => "í•­",
            Self::SubParagraph => "í˜¸",
            Self::Item => "ëª©",
        }
    }

    /// ìƒìœ„ ê³„ì¸µ ì—¬ë¶€
    pub fn is_structural(&self) -> bool {
        matches!(self, Self::Part | Self::Chapter | Self::Section | Self::SubSection)
    }
}

/// ë²•ì¡°ë¬¸ ì°¸ì¡° ì •ë³´
#[derive(Debug, Clone, Default, Serialize, Deserialize)]
pub struct LegalReference {
    pub target_law: Option<String>,
    pub target_article: Option<String>,
    #[serde(default)]
    pub reference_type: ReferenceType,
    pub raw_text: String,
}

#[derive(Debug, Clone, Copy, Default, PartialEq, Eq, Serialize, Deserialize)]
#[serde(rename_all = "snake_case")]
pub enum ReferenceType {
    #[default]
    Internal,
    External,
}

/// ë²•ë¥  ë¬¸ì„œ ë©”íƒ€ë°ì´í„°
#[derive(Debug, Clone, Default, Serialize, Deserialize)]
pub struct LegalMetadata {
    pub law_name: String,
    pub law_id: String,
    pub category: String,
    pub revision_date: Option<String>,
    pub revision_number: Option<String>,
    pub effective_date: Option<String>,

    // ê³„ì¸µ êµ¬ì¡°
    pub part: Option<String>,
    pub chapter: Option<String>,
    pub section: Option<String>,
    pub subsection: Option<String>,

    // ì¡°í•­ ì •ë³´
    pub article_number: Option<String>,
    pub article_title: Option<String>,
    pub paragraph_number: Option<String>,

    // ì°¸ì¡° ê´€ê³„
    #[serde(default)]
    pub references: Vec<LegalReference>,

    // ì›ë³¸ ìœ„ì¹˜
    pub source_file: String,
    pub line_start: usize,
    pub line_end: usize,
}

/// ì²­í¬ íƒ€ì…
#[derive(Debug, Clone, Copy, PartialEq, Eq, Serialize, Deserialize)]
#[serde(rename_all = "snake_case")]
pub enum ChunkType {
    Article,
    Paragraph,
    Definition,
}

impl Default for ChunkType {
    fn default() -> Self {
        Self::Article
    }
}

/// ë²•ë¥  ì²­í¬
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct LegalChunk {
    pub id: String,
    pub content: String,
    pub metadata: LegalMetadata,
    #[serde(default)]
    pub chunk_type: ChunkType,
    pub token_count: usize,
    pub context_path: String,
    pub parent_chunk_id: Option<String>,
}

impl LegalChunk {
    pub fn to_embedding_format(&self, include_context: bool) -> EmbeddingData {
        let enhanced_content = if include_context && !self.context_path.is_empty() {
            format!("[{}]\n\n{}", self.context_path, self.content)
        } else {
            self.content.clone()
        };

        EmbeddingData {
            id: self.id.clone(),
            content: enhanced_content,
            raw_content: self.content.clone(),
            metadata: self.to_metadata_map(),
        }
    }

    fn to_metadata_map(&self) -> EmbeddingMetadata {
        EmbeddingMetadata {
            law_name: self.metadata.law_name.clone(),
            law_id: self.metadata.law_id.clone(),
            category: self.metadata.category.clone(),
            revision_date: self.metadata.revision_date.clone(),
            effective_date: self.metadata.effective_date.clone(),
            hierarchy: HierarchyInfo {
                part: self.metadata.part.clone(),
                chapter: self.metadata.chapter.clone(),
                section: self.metadata.section.clone(),
                subsection: self.metadata.subsection.clone(),
            },
            article: ArticleInfo {
                number: self.metadata.article_number.clone(),
                title: self.metadata.article_title.clone(),
                paragraph: self.metadata.paragraph_number.clone(),
            },
            references: self.metadata.references.clone(),
            source_file: self.metadata.source_file.clone(),
            chunk_type: self.chunk_type,
            token_count: self.token_count,
            context_path: self.context_path.clone(),
        }
    }
}

/// ì„ë² ë”©ìš© ì¶œë ¥ ë°ì´í„°
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct EmbeddingData {
    pub id: String,
    pub content: String,
    pub raw_content: String,
    pub metadata: EmbeddingMetadata,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct EmbeddingMetadata {
    pub law_name: String,
    pub law_id: String,
    pub category: String,
    pub revision_date: Option<String>,
    pub effective_date: Option<String>,
    pub hierarchy: HierarchyInfo,
    pub article: ArticleInfo,
    pub references: Vec<LegalReference>,
    pub source_file: String,
    pub chunk_type: ChunkType,
    pub token_count: usize,
    pub context_path: String,
}

#[derive(Debug, Clone, Default, Serialize, Deserialize)]
pub struct HierarchyInfo {
    pub part: Option<String>,
    pub chapter: Option<String>,
    pub section: Option<String>,
    pub subsection: Option<String>,
}

#[derive(Debug, Clone, Default, Serialize, Deserialize)]
pub struct ArticleInfo {
    pub number: Option<String>,
    pub title: Option<String>,
    pub paragraph: Option<String>,
}
```

### 4.3 ì •ê·œì‹ íŒ¨í„´ (`legal/patterns.rs`)

```rust
use lazy_static::lazy_static;
use regex::Regex;
use std::collections::HashMap;

lazy_static! {
    // ===== êµ¬ì¡° íŒ¨í„´ =====

    /// í¸ íŒ¨í„´: ì œ1í¸ ì´ì¹™
    pub static ref RE_PART: Regex =
        Regex::new(r"^ì œ(\d+)í¸\s*(.*)$").unwrap();

    /// ì¥ íŒ¨í„´: ì œ1ì¥ í†µì¹™
    pub static ref RE_CHAPTER: Regex =
        Regex::new(r"^ì œ(\d+)ì¥\s*(.*)$").unwrap();

    /// ì ˆ íŒ¨í„´: ì œ1ì ˆ ëª©ì 
    pub static ref RE_SECTION: Regex =
        Regex::new(r"^ì œ(\d+)ì ˆ\s*(.*)$").unwrap();

    /// ê´€ íŒ¨í„´: ì œ1ê´€ ì •ì˜
    pub static ref RE_SUBSECTION: Regex =
        Regex::new(r"^ì œ(\d+)ê´€\s*(.*)$").unwrap();

    // ===== ì¡°í•­ íŒ¨í„´ =====

    /// ì¡° íŒ¨í„´: ì œ1ì¡°(ëª©ì ), ì œ2ì¡°ì˜2(ì •ì˜)
    pub static ref RE_ARTICLE: Regex =
        Regex::new(r"^ì œ(\d+)ì¡°(?:ì˜(\d+))?(?:\(([^)]+)\))?").unwrap();

    /// í•­ íŒ¨í„´: â‘  â‘¡ â‘¢ ë˜ëŠ” (1) (2) (3)
    pub static ref RE_PARAGRAPH: Regex =
        Regex::new(r"^([â‘ â‘¡â‘¢â‘£â‘¤â‘¥â‘¦â‘§â‘¨â‘©â‘ªâ‘«â‘¬â‘­â‘®â‘¯â‘°â‘±â‘²â‘³]|\(\d+\))\s*").unwrap();

    /// í˜¸ íŒ¨í„´: 1. 2. 3.
    pub static ref RE_SUBPARAGRAPH: Regex =
        Regex::new(r"^(\d+)\.\s*").unwrap();

    /// ëª© íŒ¨í„´: ê°€. ë‚˜. ë‹¤.
    pub static ref RE_ITEM: Regex =
        Regex::new(r"^([ê°€ë‚˜ë‹¤ë¼ë§ˆë°”ì‚¬ì•„ìì°¨ì¹´íƒ€íŒŒí•˜])\.\s*").unwrap();

    /// ì„¸ë¶€ ëª© íŒ¨í„´: (1) (2) (3)
    pub static ref RE_SUBITEM: Regex =
        Regex::new(r"^\((\d+)\)\s*").unwrap();

    // ===== ì°¸ì¡° íŒ¨í„´ =====

    /// ì™¸ë¶€ ë²•ë¥  ì°¸ì¡°: ã€Œë²•ë¥ ëª…ã€ì œ1ì¡°ì œ2í•­ì œ3í˜¸
    pub static ref RE_LAW_REFERENCE: Regex =
        Regex::new(r"ã€Œ([^ã€]+)ã€(?:\s*ì œ(\d+)ì¡°(?:ì˜(\d+))?(?:ì œ(\d+)í•­)?(?:ì œ(\d+)í˜¸)?)?").unwrap();

    /// ë‚´ë¶€ ì°¸ì¡°: ì œ1ì¡°ì œ2í•­ì œ3í˜¸ê°€ëª©
    pub static ref RE_INTERNAL_REFERENCE: Regex =
        Regex::new(r"ì œ(\d+)ì¡°(?:ì˜(\d+))?(?:ì œ(\d+)í•­)?(?:ì œ(\d+)í˜¸)?(?:([ê°€-í•˜])ëª©)?").unwrap();

    // ===== ë©”íƒ€ë°ì´í„° íŒ¨í„´ =====

    /// ê°œì • ì •ë³´: [ì¼ë¶€ê°œì • 2024. 1. 1. <ì‹œí–‰ì¼: 2024-01-01>]
    pub static ref RE_REVISION: Regex =
        Regex::new(r"\[(?:ì¼ë¶€)?ê°œì •\s*(\d{4})\.\s*(\d{1,2})\.\s*(\d{1,2}).*?(?:<ì‹œí–‰ì¼\s*:\s*(\d{4}-\d{2}-\d{2})>)?\]").unwrap();

    /// ê°œì • ì°¨ìˆ˜: ì œ5ì°¨ ì¼ë¶€ê°œì •
    pub static ref RE_REVISION_NUMBER: Regex =
        Regex::new(r"ì œ(\d+)ì°¨\s*(?:ì¼ë¶€)?ê°œì •").unwrap();

    // ===== ë§¤í•‘ í…Œì´ë¸” =====

    /// ì›ë¬¸ì â†’ ìˆ«ì ë§¤í•‘
    pub static ref CIRCLED_NUMBERS: HashMap<char, u8> = {
        let mut m = HashMap::new();
        m.insert('â‘ ', 1);  m.insert('â‘¡', 2);  m.insert('â‘¢', 3);
        m.insert('â‘£', 4);  m.insert('â‘¤', 5);  m.insert('â‘¥', 6);
        m.insert('â‘¦', 7);  m.insert('â‘§', 8);  m.insert('â‘¨', 9);
        m.insert('â‘©', 10); m.insert('â‘ª', 11); m.insert('â‘«', 12);
        m.insert('â‘¬', 13); m.insert('â‘­', 14); m.insert('â‘®', 15);
        m.insert('â‘¯', 16); m.insert('â‘°', 17); m.insert('â‘±', 18);
        m.insert('â‘²', 19); m.insert('â‘³', 20);
        m
    };

    /// í•œê¸€ ëª© ë¬¸ì ë°°ì—´
    pub static ref KOREAN_ITEMS: [char; 14] = [
        'ê°€', 'ë‚˜', 'ë‹¤', 'ë¼', 'ë§ˆ', 'ë°”', 'ì‚¬',
        'ì•„', 'ì', 'ì°¨', 'ì¹´', 'íƒ€', 'íŒŒ', 'í•˜'
    ];
}

/// ì›ë¬¸ìë¥¼ ìˆ«ìë¡œ ë³€í™˜
pub fn circled_to_number(c: char) -> Option<u8> {
    CIRCLED_NUMBERS.get(&c).copied()
}

/// í•œê¸€ ëª© ë¬¸ìì˜ ì¸ë±ìŠ¤ ë°˜í™˜
pub fn korean_item_index(c: char) -> Option<usize> {
    KOREAN_ITEMS.iter().position(|&x| x == c)
}

#[cfg(test)]
mod tests {
    use super::*;

    #[test]
    fn test_article_pattern() {
        let cases = [
            ("ì œ1ì¡°(ëª©ì )", Some(("1", None, Some("ëª©ì ")))),
            ("ì œ2ì¡°ì˜2(ì •ì˜)", Some(("2", Some("2"), Some("ì •ì˜")))),
            ("ì œ10ì¡°", Some(("10", None, None))),
        ];

        for (input, expected) in cases {
            let caps = RE_ARTICLE.captures(input);
            match expected {
                Some((num, sub, title)) => {
                    let c = caps.expect("Should match");
                    assert_eq!(c.get(1).map(|m| m.as_str()), Some(num));
                    assert_eq!(c.get(2).map(|m| m.as_str()), sub);
                    assert_eq!(c.get(3).map(|m| m.as_str()), title);
                }
                None => assert!(caps.is_none()),
            }
        }
    }

    #[test]
    fn test_circled_numbers() {
        assert_eq!(circled_to_number('â‘ '), Some(1));
        assert_eq!(circled_to_number('â‘©'), Some(10));
        assert_eq!(circled_to_number('â‘³'), Some(20));
        assert_eq!(circled_to_number('A'), None);
    }

    #[test]
    fn test_korean_items() {
        assert_eq!(korean_item_index('ê°€'), Some(0));
        assert_eq!(korean_item_index('ë‚˜'), Some(1));
        assert_eq!(korean_item_index('í•˜'), Some(13));
        assert_eq!(korean_item_index('í£'), None);
    }
}
```

### 4.4 ì²­ì»¤ êµ¬í˜„ (`legal/chunker.rs`)

```rust
use crate::legal::patterns::*;
use crate::legal::types::*;
use sha2::{Sha256, Digest};
use std::collections::HashMap;
use std::path::Path;
use std::fs;

/// í•œêµ­ ë²•ë¥  ë¬¸ì„œ ì²­ì»¤
pub struct KoreanLegalChunker {
    /// ì¡°(Article) ë‹¨ìœ„ë¡œ ì²­í‚¹í• ì§€ ì—¬ë¶€
    pub chunk_by_article: bool,
    /// ìƒìœ„ ê³„ì¸µ ì»¨í…ìŠ¤íŠ¸ í¬í•¨ ì—¬ë¶€
    pub include_context: bool,
    /// ìµœëŒ€ ì²­í¬ í† í° ìˆ˜
    pub max_chunk_tokens: usize,
    /// ì²­í¬ ê°„ ì˜¤ë²„ë© í† í° ìˆ˜
    pub overlap_tokens: usize,

    /// í˜„ì¬ íŒŒì‹± ìƒíƒœ
    current_state: ParsingState,
}

#[derive(Debug, Clone, Default)]
struct ParsingState {
    part: Option<String>,
    chapter: Option<String>,
    section: Option<String>,
    subsection: Option<String>,
    article: Option<String>,
    paragraph: Option<String>,
}

impl Default for KoreanLegalChunker {
    fn default() -> Self {
        Self {
            chunk_by_article: true,
            include_context: true,
            max_chunk_tokens: 512,
            overlap_tokens: 50,
            current_state: ParsingState::default(),
        }
    }
}

impl KoreanLegalChunker {
    pub fn new() -> Self {
        Self::default()
    }

    pub fn with_max_tokens(mut self, tokens: usize) -> Self {
        self.max_chunk_tokens = tokens;
        self
    }

    /// í† í° ìˆ˜ ì¶”ì • (í•œê¸€ì€ ëŒ€ëµ 1.5ìë‹¹ 1í† í°)
    pub fn estimate_tokens(&self, text: &str) -> usize {
        let korean: usize = text.chars()
            .filter(|c| ('\u{AC00}'..='\u{D7A3}').contains(c))
            .count();
        let alphanumeric: usize = text.chars()
            .filter(|c| c.is_ascii_alphanumeric())
            .count();
        let spaces: usize = text.chars()
            .filter(|c| c.is_whitespace())
            .count();

        (korean as f64 / 1.5 + alphanumeric as f64 / 4.0 + spaces as f64 / 4.0) as usize
    }

    /// ì²­í¬ ê³ ìœ  ID ìƒì„± (SHA256 ê¸°ë°˜)
    fn generate_chunk_id(&self, content: &str, metadata: &LegalMetadata) -> String {
        let unique_str = format!(
            "{}:{}:{}",
            metadata.law_name,
            metadata.article_number.as_deref().unwrap_or(""),
            &content[..content.len().min(100)]
        );

        let mut hasher = Sha256::new();
        hasher.update(unique_str.as_bytes());
        let result = hasher.finalize();
        hex::encode(&result[..8])  // 16ì hex
    }

    /// ë§ˆí¬ë‹¤ìš´ í—¤ë”ì—ì„œ ë©”íƒ€ë°ì´í„° ì¶”ì¶œ
    fn parse_metadata_header(&self, lines: &[&str]) -> (LegalMetadata, usize) {
        let mut metadata = LegalMetadata::default();
        let mut body_start = 0;

        for (i, line) in lines.iter().enumerate() {
            let line = line.trim();

            // ì œëª© (# ë²•ë ¹ëª…)
            if line.starts_with("# ") {
                metadata.law_name = line[2..].trim().to_string();
                continue;
            }

            // ê·œì • ID
            if line.starts_with("- **ê·œì • ID**:") || line.starts_with("- **ê·œì •ID**:") {
                metadata.law_id = line.split(':').last().unwrap_or("").trim().to_string();
                continue;
            }

            // ë¶„ë¥˜
            if line.starts_with("- **ë¶„ë¥˜**:") {
                metadata.category = line.split(':').last().unwrap_or("").trim().to_string();
                continue;
            }

            // ê°œì • ì •ë³´
            if let Some(caps) = RE_REVISION.captures(line) {
                if let (Some(y), Some(m), Some(d)) = (caps.get(1), caps.get(2), caps.get(3)) {
                    metadata.revision_date = Some(format!(
                        "{}-{:0>2}-{:0>2}",
                        y.as_str(),
                        m.as_str(),
                        d.as_str()
                    ));
                }
                if let Some(eff) = caps.get(4) {
                    metadata.effective_date = Some(eff.as_str().to_string());
                }
            }

            // ê°œì • ì°¨ìˆ˜
            if let Some(caps) = RE_REVISION_NUMBER.captures(line) {
                if let Some(num) = caps.get(1) {
                    metadata.revision_number = Some(num.as_str().to_string());
                }
            }

            // ë³¸ë¬¸ ì‹œì‘ ê°ì§€
            if RE_PART.is_match(line) || RE_CHAPTER.is_match(line) || RE_ARTICLE.is_match(line) {
                body_start = i;
                break;
            }
        }

        (metadata, body_start)
    }

    /// ë²•ì¡°ë¬¸ ì°¸ì¡° ê´€ê³„ ì¶”ì¶œ
    fn extract_references(&self, text: &str) -> Vec<LegalReference> {
        let mut references = Vec::new();

        // ì™¸ë¶€ ë²•ë¥  ì°¸ì¡°
        for caps in RE_LAW_REFERENCE.captures_iter(text) {
            let target_article = caps.get(2).map(|m| {
                let mut article = format!("ì œ{}ì¡°", m.as_str());
                if let Some(sub) = caps.get(3) {
                    article.push_str(&format!("ì˜{}", sub.as_str()));
                }
                article
            });

            references.push(LegalReference {
                target_law: caps.get(1).map(|m| m.as_str().to_string()),
                target_article,
                reference_type: ReferenceType::External,
                raw_text: caps.get(0).unwrap().as_str().to_string(),
            });
        }

        // ë‚´ë¶€ ì°¸ì¡° (ì´ë¯¸ ì™¸ë¶€ ì°¸ì¡°ë¡œ ì²˜ë¦¬ëœ ê²ƒ ì œì™¸)
        for caps in RE_INTERNAL_REFERENCE.captures_iter(text) {
            let raw = caps.get(0).unwrap().as_str();
            if references.iter().any(|r| r.raw_text.contains(raw)) {
                continue;
            }

            let mut article = format!("ì œ{}ì¡°", caps.get(1).unwrap().as_str());
            if let Some(sub) = caps.get(2) {
                article.push_str(&format!("ì˜{}", sub.as_str()));
            }

            references.push(LegalReference {
                target_law: None,
                target_article: Some(article),
                reference_type: ReferenceType::Internal,
                raw_text: raw.to_string(),
            });
        }

        references
    }

    /// í˜„ì¬ ê³„ì¸µ ê²½ë¡œ ë¬¸ìì—´ ìƒì„±
    fn build_context_path(&self) -> String {
        let mut parts = Vec::new();

        if let Some(ref p) = self.current_state.part {
            parts.push(p.clone());
        }
        if let Some(ref c) = self.current_state.chapter {
            parts.push(c.clone());
        }
        if let Some(ref s) = self.current_state.section {
            parts.push(s.clone());
        }
        if let Some(ref ss) = self.current_state.subsection {
            parts.push(ss.clone());
        }
        if let Some(ref a) = self.current_state.article {
            parts.push(a.clone());
        }

        parts.join(" > ")
    }

    /// ë§ˆí¬ë‹¤ìš´ íŒŒì¼ íŒŒì‹±
    pub fn parse_markdown(&mut self, filepath: &Path) -> Result<Vec<LegalChunk>, std::io::Error> {
        let content = fs::read_to_string(filepath)?;
        let lines: Vec<&str> = content.lines().collect();

        // ë©”íƒ€ë°ì´í„° í—¤ë” íŒŒì‹±
        let (mut base_metadata, body_start) = self.parse_metadata_header(&lines);
        base_metadata.source_file = filepath
            .file_name()
            .map(|n| n.to_string_lossy().to_string())
            .unwrap_or_default();

        // ìƒíƒœ ì´ˆê¸°í™”
        self.current_state = ParsingState::default();

        let mut chunks = Vec::new();
        let mut current_idx = body_start;

        while current_idx < lines.len() {
            let line = lines[current_idx].trim();

            // ë¹ˆ ì¤„ ìŠ¤í‚µ
            if line.is_empty() {
                current_idx += 1;
                continue;
            }

            // í¸/ì¥/ì ˆ/ê´€ ì—…ë°ì´íŠ¸
            if let Some(caps) = RE_PART.captures(line) {
                self.current_state.part = Some(format!(
                    "ì œ{}í¸ {}",
                    caps.get(1).unwrap().as_str(),
                    caps.get(2).map(|m| m.as_str()).unwrap_or("")
                ).trim().to_string());
                self.current_state.chapter = None;
                self.current_state.section = None;
                self.current_state.subsection = None;
                current_idx += 1;
                continue;
            }

            if let Some(caps) = RE_CHAPTER.captures(line) {
                self.current_state.chapter = Some(format!(
                    "ì œ{}ì¥ {}",
                    caps.get(1).unwrap().as_str(),
                    caps.get(2).map(|m| m.as_str()).unwrap_or("")
                ).trim().to_string());
                self.current_state.section = None;
                self.current_state.subsection = None;
                current_idx += 1;
                continue;
            }

            if let Some(caps) = RE_SECTION.captures(line) {
                self.current_state.section = Some(format!(
                    "ì œ{}ì ˆ {}",
                    caps.get(1).unwrap().as_str(),
                    caps.get(2).map(|m| m.as_str()).unwrap_or("")
                ).trim().to_string());
                self.current_state.subsection = None;
                current_idx += 1;
                continue;
            }

            if let Some(caps) = RE_SUBSECTION.captures(line) {
                self.current_state.subsection = Some(format!(
                    "ì œ{}ê´€ {}",
                    caps.get(1).unwrap().as_str(),
                    caps.get(2).map(|m| m.as_str()).unwrap_or("")
                ).trim().to_string());
                current_idx += 1;
                continue;
            }

            // ì¡° íŒŒì‹±
            if RE_ARTICLE.is_match(line) {
                let (chunk, next_idx) = self.parse_article_block(&lines, current_idx, &base_metadata);
                if !chunk.content.is_empty() {
                    chunks.push(chunk);
                }
                current_idx = next_idx;
            } else {
                current_idx += 1;
            }
        }

        Ok(chunks)
    }

    /// ì¡°(Article) ë¸”ë¡ íŒŒì‹±
    fn parse_article_block(
        &mut self,
        lines: &[&str],
        start_idx: usize,
        base_metadata: &LegalMetadata,
    ) -> (LegalChunk, usize) {
        let mut article_lines = Vec::new();
        let mut current_idx = start_idx;

        // ì²« ì¤„ì—ì„œ ì¡° ì •ë³´ ì¶”ì¶œ
        let first_line = lines[start_idx].trim();
        let article_caps = RE_ARTICLE.captures(first_line);

        let (article_num, article_title) = if let Some(caps) = &article_caps {
            let mut num = format!("ì œ{}ì¡°", caps.get(1).unwrap().as_str());
            if let Some(sub) = caps.get(2) {
                num.push_str(&format!("ì˜{}", sub.as_str()));
            }
            let title = caps.get(3).map(|m| m.as_str().to_string());

            self.current_state.article = Some(if let Some(ref t) = title {
                format!("{}({})", num, t)
            } else {
                num.clone()
            });

            (Some(num), title)
        } else {
            (None, None)
        };

        // ë‹¤ìŒ ì¡°ê°€ ë‚˜ì˜¬ ë•Œê¹Œì§€ ìˆ˜ì§‘
        while current_idx < lines.len() {
            let line = lines[current_idx].trim();

            // ë‹¤ìŒ ì¡° ì‹œì‘ ê°ì§€
            if current_idx > start_idx && RE_ARTICLE.is_match(line) {
                break;
            }

            // í¸/ì¥/ì ˆ/ê´€ ê°ì§€ ì‹œ ìƒíƒœ ì—…ë°ì´íŠ¸ í›„ ê³„ì†
            if RE_PART.is_match(line) || RE_CHAPTER.is_match(line)
                || RE_SECTION.is_match(line) || RE_SUBSECTION.is_match(line) {
                // ì´ë¯¸ ìƒìœ„ ë£¨í”„ì—ì„œ ì²˜ë¦¬í•˜ë¯€ë¡œ ì—¬ê¸°ì„œëŠ” break
                break;
            }

            article_lines.push(line);
            current_idx += 1;
        }

        let content = article_lines.join("\n").trim().to_string();

        // ë©”íƒ€ë°ì´í„° ìƒì„±
        let mut chunk_metadata = base_metadata.clone();
        chunk_metadata.part = self.current_state.part.clone();
        chunk_metadata.chapter = self.current_state.chapter.clone();
        chunk_metadata.section = self.current_state.section.clone();
        chunk_metadata.subsection = self.current_state.subsection.clone();
        chunk_metadata.article_number = article_num;
        chunk_metadata.article_title = article_title;
        chunk_metadata.references = self.extract_references(&content);
        chunk_metadata.line_start = start_idx;
        chunk_metadata.line_end = current_idx.saturating_sub(1);

        let chunk = LegalChunk {
            id: self.generate_chunk_id(&content, &chunk_metadata),
            content: content.clone(),
            metadata: chunk_metadata,
            chunk_type: ChunkType::Article,
            token_count: self.estimate_tokens(&content),
            context_path: self.build_context_path(),
            parent_chunk_id: None,
        };

        (chunk, current_idx)
    }

    /// í° ì¡°ë¬¸ì„ í•­(Paragraph) ë‹¨ìœ„ë¡œ ë¶„í• 
    pub fn chunk_large_article(&self, chunk: LegalChunk) -> Vec<LegalChunk> {
        if chunk.token_count <= self.max_chunk_tokens {
            return vec![chunk];
        }

        let mut sub_chunks = Vec::new();
        let lines: Vec<&str> = chunk.content.lines().collect();
        let mut current_content = Vec::new();
        let mut current_paragraph: Option<String> = None;

        for line in lines {
            if let Some(caps) = RE_PARAGRAPH.captures(line) {
                // ì´ì „ í•­ ì €ì¥
                if !current_content.is_empty() {
                    let content = current_content.join("\n").trim().to_string();
                    if !content.is_empty() {
                        let mut sub_meta = chunk.metadata.clone();
                        sub_meta.paragraph_number = current_paragraph.clone();

                        sub_chunks.push(LegalChunk {
                            id: self.generate_chunk_id(&content, &sub_meta),
                            content,
                            metadata: sub_meta,
                            chunk_type: ChunkType::Paragraph,
                            token_count: self.estimate_tokens(&current_content.join("\n")),
                            context_path: chunk.context_path.clone(),
                            parent_chunk_id: Some(chunk.id.clone()),
                        });
                    }
                    current_content.clear();
                }

                // ì›ë¬¸ìë¥¼ ìˆ«ìë¡œ ë³€í™˜
                let circled = caps.get(1).unwrap().as_str();
                current_paragraph = circled.chars().next()
                    .and_then(circled_to_number)
                    .map(|n| n.to_string())
                    .or_else(|| Some(circled.trim_matches(|c| c == '(' || c == ')').to_string()));
            }

            current_content.push(line);
        }

        // ë§ˆì§€ë§‰ í•­ ì €ì¥
        if !current_content.is_empty() {
            let content = current_content.join("\n").trim().to_string();
            if !content.is_empty() {
                let mut sub_meta = chunk.metadata.clone();
                sub_meta.paragraph_number = current_paragraph;

                sub_chunks.push(LegalChunk {
                    id: self.generate_chunk_id(&content, &sub_meta),
                    content,
                    metadata: sub_meta,
                    chunk_type: ChunkType::Paragraph,
                    token_count: self.estimate_tokens(&current_content.join("\n")),
                    context_path: chunk.context_path.clone(),
                    parent_chunk_id: Some(chunk.id.clone()),
                });
            }
        }

        if sub_chunks.is_empty() {
            vec![chunk]
        } else {
            sub_chunks
        }
    }
}

#[cfg(test)]
mod tests {
    use super::*;

    #[test]
    fn test_estimate_tokens() {
        let chunker = KoreanLegalChunker::new();

        // í•œê¸€ 10ì â‰ˆ 6.67 í† í°
        assert!(chunker.estimate_tokens("ì•ˆë…•í•˜ì„¸ìš”í…ŒìŠ¤íŠ¸ì…ë‹ˆë‹¤") < 10);

        // ì˜ë¬¸ 10ì â‰ˆ 2.5 í† í°
        assert!(chunker.estimate_tokens("helloworld") < 5);
    }

    #[test]
    fn test_build_context_path() {
        let mut chunker = KoreanLegalChunker::new();
        chunker.current_state.part = Some("ì œ1í¸ ì´ì¹™".to_string());
        chunker.current_state.chapter = Some("ì œ1ì¥ í†µì¹™".to_string());
        chunker.current_state.article = Some("ì œ1ì¡°(ëª©ì )".to_string());

        let path = chunker.build_context_path();
        assert_eq!(path, "ì œ1í¸ ì´ì¹™ > ì œ1ì¥ í†µì¹™ > ì œ1ì¡°(ëª©ì )");
    }
}
```

### 4.5 ë‚´ë³´ë‚´ê¸° (`legal/exporter.rs`)

```rust
use crate::legal::types::*;
use std::fs::File;
use std::io::{BufWriter, Write};
use std::path::Path;

/// WeKnora RAG ì‹œìŠ¤í…œìš© ë‚´ë³´ë‚´ê¸°
pub struct WeKnoraExporter;

impl WeKnoraExporter {
    /// ì„ë² ë”©ìš© ë°ì´í„° ë³€í™˜
    pub fn export_for_embedding(
        chunks: &[LegalChunk],
        include_context_in_content: bool,
    ) -> Vec<EmbeddingData> {
        chunks.iter()
            .map(|chunk| chunk.to_embedding_format(include_context_in_content))
            .collect()
    }

    /// JSONL íŒŒì¼ë¡œ ë‚´ë³´ë‚´ê¸°
    pub fn export_to_jsonl(
        chunks: &[LegalChunk],
        output_path: &Path,
        include_context_in_content: bool,
    ) -> Result<usize, std::io::Error> {
        let data = Self::export_for_embedding(chunks, include_context_in_content);

        let file = File::create(output_path)?;
        let mut writer = BufWriter::new(file);

        for item in &data {
            let json = serde_json::to_string(item)?;
            writeln!(writer, "{}", json)?;
        }

        writer.flush()?;
        Ok(data.len())
    }
}

#[cfg(test)]
mod tests {
    use super::*;
    use tempfile::tempdir;

    #[test]
    fn test_export_to_jsonl() {
        let chunk = LegalChunk {
            id: "test123".to_string(),
            content: "í…ŒìŠ¤íŠ¸ ë‚´ìš©".to_string(),
            metadata: LegalMetadata {
                law_name: "í…ŒìŠ¤íŠ¸ë²•".to_string(),
                ..Default::default()
            },
            chunk_type: ChunkType::Article,
            token_count: 10,
            context_path: "ì œ1ì¡°".to_string(),
            parent_chunk_id: None,
        };

        let dir = tempdir().unwrap();
        let output_path = dir.path().join("test.jsonl");

        let count = WeKnoraExporter::export_to_jsonl(
            &[chunk],
            &output_path,
            true,
        ).unwrap();

        assert_eq!(count, 1);
        assert!(output_path.exists());
    }
}
```

---

## 5. í…ŒìŠ¤íŠ¸ ì „ëµ

### 5.1 ë‹¨ìœ„ í…ŒìŠ¤íŠ¸

| ëª¨ë“ˆ | í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ |
|------|--------------|
| `record.rs` | `test_extended_ctrl_char_0x16_to_0x1f` |
| `record.rs` | `test_invalid_surrogate_replacement` |
| `patterns.rs` | `test_article_pattern` |
| `patterns.rs` | `test_circled_numbers` |
| `chunker.rs` | `test_estimate_tokens` |
| `chunker.rs` | `test_build_context_path` |
| `exporter.rs` | `test_export_to_jsonl` |

### 5.2 í†µí•© í…ŒìŠ¤íŠ¸

1. **HWP í…ìŠ¤íŠ¸ ì¶”ì¶œ í…ŒìŠ¤íŠ¸**
   - í™•ì¥ ì œì–´ ë¬¸ì í¬í•¨ íŒŒì¼ë¡œ í…ìŠ¤íŠ¸ ì¶”ì¶œ
   - Python ì¶œë ¥ê³¼ ë¹„êµ

2. **ë²•ë¥  ì²­í‚¹ í…ŒìŠ¤íŠ¸**
   - ì‹¤ì œ ë²•ë¥  ë§ˆí¬ë‹¤ìš´ íŒŒì‹±
   - ì²­í¬ ìˆ˜, í† í° ìˆ˜ ê²€ì¦

3. **JSONL ì¶œë ¥ í…ŒìŠ¤íŠ¸**
   - ì¶œë ¥ íŒŒì¼ í˜•ì‹ ê²€ì¦
   - WeKnora API í˜¸í™˜ì„± í™•ì¸

---

## 6. ë§ˆì´ê·¸ë ˆì´ì…˜ ê³„íš

### Phase 1: í•µì‹¬ ë²„ê·¸ ìˆ˜ì • (1ì¼)
- `record.rs` ìˆ˜ì •
- í…ŒìŠ¤íŠ¸ ì¶”ê°€ ë° ê²€ì¦

### Phase 2: ë²•ë¥  íŒŒì„œ êµ¬í˜„ (3ì¼)
- íƒ€ì… ì •ì˜
- ì •ê·œì‹ íŒ¨í„´
- ì²­ì»¤ êµ¬í˜„
- ë‚´ë³´ë‚´ê¸°

### Phase 3: CLI í†µí•© (1ì¼)
- ëª…ë ¹ì–´ ì¶”ê°€
- ì‚¬ìš© ë¬¸ì„œ ì‘ì„±

### Phase 4: ê²€ì¦ ë° ë°°í¬ (1ì¼)
- Python ì¶œë ¥ê³¼ ë¹„êµ í…ŒìŠ¤íŠ¸
- ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬
- ë¬¸ì„œí™”

---

## 7. ì°¸ê³  ìë£Œ

### ì†ŒìŠ¤ íŒŒì¼
- Python íŒŒì„œ: `/Users/seunghan/krx_listing/krx_law/legal_chunker.py`
- Python HWP ë³€í™˜ê¸°: `/Users/seunghan/krx_listing/tmp/markdown-media/converters/hwp_converter.py`
- Rust ë ˆì½”ë“œ íŒŒì„œ: `/Users/seunghan/krx_listing/tmp/markdown-media/core/src/hwp/record.rs`

### ì™¸ë¶€ ë¬¸ì„œ
- [HWP 5.0 íŒŒì¼ êµ¬ì¡°](https://www.hancom.com/cs_center/csFaqView.do)
- [Rust regex í¬ë ˆì´íŠ¸](https://docs.rs/regex/latest/regex/)
- [WeKnora API ë¬¸ì„œ](https://weknora.com/docs)
